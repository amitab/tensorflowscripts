{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             pixel1        pixel2        pixel3        pixel4        pixel5  \\\n",
      "count  60000.000000  60000.000000  60000.000000  60000.000000  60000.000000   \n",
      "mean       0.000900      0.006150      0.035333      0.101933      0.247967   \n",
      "std        0.094689      0.271011      1.222324      2.452871      4.306912   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max       16.000000     36.000000    226.000000    164.000000    227.000000   \n",
      "\n",
      "             pixel6        pixel7        pixel8        pixel9       pixel10  \\\n",
      "count  60000.000000  60000.000000  60000.000000  60000.000000  60000.000000   \n",
      "mean       0.411467      0.805767      2.198283      5.682000     14.488767   \n",
      "std        5.836188      8.215169     14.093378     23.819481     38.334549   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max      230.000000    224.000000    255.000000    254.000000    255.000000   \n",
      "\n",
      "       ...      pixel775      pixel776      pixel777      pixel778  \\\n",
      "count  ...  60000.000000  60000.000000  60000.000000  60000.000000   \n",
      "mean   ...     34.625400     23.300683     16.588267     17.869433   \n",
      "std    ...     57.545242     48.854427     41.979611     43.966032   \n",
      "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
      "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
      "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
      "75%    ...     58.000000      9.000000      0.000000      0.000000   \n",
      "max    ...    255.000000    255.000000    255.000000    255.000000   \n",
      "\n",
      "           pixel779      pixel780      pixel781      pixel782      pixel783  \\\n",
      "count  60000.000000  60000.000000  60000.000000  60000.000000  60000.000000   \n",
      "mean      22.814817     17.911483      8.520633      2.753300      0.855517   \n",
      "std       51.830477     45.149388     29.614859     17.397652      9.356960   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max      255.000000    255.000000    255.000000    255.000000    255.000000   \n",
      "\n",
      "          pixel784  \n",
      "count  60000.00000  \n",
      "mean       0.07025  \n",
      "std        2.12587  \n",
      "min        0.00000  \n",
      "25%        0.00000  \n",
      "50%        0.00000  \n",
      "75%        0.00000  \n",
      "max      170.00000  \n",
      "\n",
      "[8 rows x 784 columns]\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/kemalty/feed-forward-n-n-with-keras-for-fashion-mnist/notebook?select=fashion-mnist_train.csv\n",
    "\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Loading and pre-processing training dataset\n",
    "train_data = pd.read_csv(\"data/fashion-mnist_train.csv\")\n",
    "train_label = pd.DataFrame(train_data[[\"label\"]].copy(deep=False)) # Seperate labels (y) from inputs (X)\n",
    "train_input = pd.DataFrame(train_data.drop(\"label\", 1, inplace=False))\n",
    "del train_data\n",
    "\n",
    "# Convert labels to dummies (one-hot encoding) so that they can be used in the output layer\n",
    "train_label = to_categorical(train_label)\n",
    "\n",
    "print(train_input.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0             1             2             3             4  \\\n",
      "count  60000.000000  60000.000000  60000.000000  60000.000000  60000.000000   \n",
      "mean       0.100000      0.100000      0.100000      0.100000      0.100000   \n",
      "std        0.299937      0.299937      0.299937      0.299936      0.299937   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "                  5             6             7             8             9  \n",
      "count  60000.000000  60000.000000  60000.000000  60000.000000  60000.000000  \n",
      "mean       0.100000      0.100000      0.100000      0.100000      0.100000  \n",
      "std        0.299934      0.299934      0.299934      0.299936      0.299936  \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of labels\n",
    "import pandas as pd\n",
    "print(pd.DataFrame(train_label).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means:\n",
      "pixel1    0.000900\n",
      "pixel2    0.006150\n",
      "pixel3    0.035333\n",
      "pixel4    0.101933\n",
      "pixel5    0.247967\n",
      "dtype: float64\n",
      "Stds:\n",
      "pixel1    0.094689\n",
      "pixel2    0.271011\n",
      "pixel3    1.222324\n",
      "pixel4    2.452871\n",
      "pixel5    4.306912\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Normalize the inputs\n",
    "train_means = train_input.mean(axis=0) # Keep these for test too\n",
    "train_stds  = train_input.std(axis=0)\n",
    "print(\"Means:\")\n",
    "print(train_means.head(5))\n",
    "print(\"Stds:\")\n",
    "print(train_stds.head(5))\n",
    "\n",
    "train_input = train_input - train_means # Zero mean\n",
    "train_input = train_input / train_stds # 1 standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0             1             2             3             4  \\\n",
      "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
      "mean       0.100000      0.100000      0.100000      0.100000      0.100000   \n",
      "std        0.300024      0.300024      0.300024      0.300025      0.300024   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "                  5             6             7             8             9  \n",
      "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000  \n",
      "mean       0.100000      0.100000      0.100000      0.100000      0.100000  \n",
      "std        0.300024      0.300024      0.300024      0.300024      0.300024  \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Loading and pre-processing testing dataset\n",
    "test_data = pd.read_csv(\"data/fashion-mnist_test.csv\") # Load the csv from file\n",
    "test_label = pd.DataFrame(test_data[[\"label\"]].copy(deep=False)) # Seperate labels (y) from inputs (X)\n",
    "test_input = pd.DataFrame(test_data.drop(\"label\", 1, inplace=False))\n",
    "del test_data\n",
    "\n",
    "# Convert labels to dummies (one-hot encoding) so that they can be used in the output layer\n",
    "test_label = to_categorical(test_label)\n",
    "\n",
    "print(pd.DataFrame(test_label).describe())\n",
    "\n",
    "# Apply normalization\n",
    "test_input = test_input - train_means # Zero mean\n",
    "test_input = test_input / train_stds # 1 standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_56 (Dense)             (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 300)               150300    \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 10)                260       \n",
      "=================================================================\n",
      "Total params: 629,685\n",
      "Trainable params: 629,685\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Set-up the network\n",
    "model = Sequential()\n",
    "model.add(Dense(units=500, input_dim=train_input.shape[1],\n",
    "                activation=\"relu\",\n",
    "                 kernel_initializer=\"random_uniform\",\n",
    "                 bias_initializer=\"zeros\"))\n",
    "model.add(Dropout(0.30))\n",
    "model.add(Dense(units=300, activation=\"relu\", kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units=200, activation=\"relu\", kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\"))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\"))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(units=50, activation=\"relu\", kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\"))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(units=25, activation=\"relu\", kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "# Print out the network configuration\n",
    "print(model.summary())\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X_train = train_input.to_numpy()\n",
    "y_train = train_label\n",
    "\n",
    "shared_percent = 25 / 100\n",
    "shared_train_data_size = int(X_train.shape[0] * shared_percent)\n",
    "num_models = 2\n",
    "\n",
    "shared_train_data = X_train[:shared_train_data_size]\n",
    "shared_train_labels = y_train[:shared_train_data_size]\n",
    "\n",
    "private_train_data = np.split(X_train[shared_train_data_size:], num_models)\n",
    "private_train_labels = np.split(\n",
    "    y_train[shared_train_data_size:], num_models)\n",
    "\n",
    "og_file_name = 'models/main_ff_{}_{}.h5'.format(int(shared_percent * 100), num_models)\n",
    "model.save(og_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model - 0\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 112ms/step - loss: 2.1693 - accuracy: 0.1868\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 1.6841 - accuracy: 0.3163\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 1.3246 - accuracy: 0.4442\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 1.2181 - accuracy: 0.4905\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 1.0391 - accuracy: 0.5670\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 1s 110ms/step - loss: 0.9729 - accuracy: 0.5954\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 0.9260 - accuracy: 0.6227\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 1s 116ms/step - loss: 0.8625 - accuracy: 0.6630\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 1s 112ms/step - loss: 0.8115 - accuracy: 0.6937\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 1s 123ms/step - loss: 0.7262 - accuracy: 0.7324\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 1s 114ms/step - loss: 0.6907 - accuracy: 0.7459\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 1s 134ms/step - loss: 0.6810 - accuracy: 0.7553\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 1s 112ms/step - loss: 0.6463 - accuracy: 0.7686\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 1s 110ms/step - loss: 0.6061 - accuracy: 0.7904\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 0.5984 - accuracy: 0.7903\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 0.5885 - accuracy: 0.7972\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 1s 111ms/step - loss: 0.5442 - accuracy: 0.8115\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 0.5318 - accuracy: 0.8165\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 1s 110ms/step - loss: 0.5537 - accuracy: 0.8094\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 1s 102ms/step - loss: 0.5148 - accuracy: 0.8235\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5733 - accuracy: 0.8037\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5688 - accuracy: 0.8036\n",
      "\n",
      "Test Accuracy:0.8036999702453613\n",
      "Train Accuracy:0.8035500049591064\n",
      "Training model - 1\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 2.2271 - accuracy: 0.1957\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 1.6676 - accuracy: 0.3150\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 1.3094 - accuracy: 0.4641\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 1s 115ms/step - loss: 1.0891 - accuracy: 0.5365\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 1.0006 - accuracy: 0.5763\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 0.9428 - accuracy: 0.6136\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 0.8431 - accuracy: 0.6701\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 0.7770 - accuracy: 0.7082\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 0.7216 - accuracy: 0.7372\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 0.7269 - accuracy: 0.7330\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 0.6546 - accuracy: 0.7595\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 1s 109ms/step - loss: 0.6508 - accuracy: 0.7663\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 1s 110ms/step - loss: 0.6342 - accuracy: 0.7742\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 0.5952 - accuracy: 0.7894\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 1s 110ms/step - loss: 0.5840 - accuracy: 0.7952\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 0.5852 - accuracy: 0.7973\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 1s 111ms/step - loss: 0.5535 - accuracy: 0.8080\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 1s 113ms/step - loss: 0.5407 - accuracy: 0.8153\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 1s 111ms/step - loss: 0.5435 - accuracy: 0.8138\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 1s 109ms/step - loss: 0.5171 - accuracy: 0.8232\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4817 - accuracy: 0.8292\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4705 - accuracy: 0.8336\n",
      "\n",
      "Test Accuracy:0.829200029373169\n",
      "Train Accuracy:0.8335833549499512\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "import keras\n",
    "\n",
    "for i in range(num_models):\n",
    "    print(\"Training model - {}\".format(i))\n",
    "    \n",
    "    model_train_data = np.vstack((shared_train_data, private_train_data[i]))\n",
    "    model_train_labels = np.concatenate(\n",
    "        (shared_train_labels, private_train_labels[i]))\n",
    "\n",
    "    model = keras.models.load_model(og_file_name)\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=\"RMSprop\",#lr=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(model_train_data, model_train_labels, epochs=20, batch_size=6000)\n",
    "    \n",
    "    model_name = \"models/model_ff_{}_{}_{}.h5\".format(i, int(shared_percent * 100), num_models)\n",
    "    model.save(model_name)\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss_and_metrics = model.evaluate(test_input.to_numpy(), test_label)\n",
    "    train_loss_and_metrics = model.evaluate(train_input.to_numpy(), train_label)\n",
    "    print(\"\")\n",
    "    print(\"Test Accuracy:\" + str(test_loss_and_metrics[1]))\n",
    "    print(\"Train Accuracy:\" + str(train_loss_and_metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def split(array, nrows, ncols):\n",
    "    \"\"\"Split a matrix into sub-matrices.\"\"\"\n",
    "    r, h = array.shape\n",
    "    if r % nrows != 0:\n",
    "        padding = (math.ceil(r / nrows) * nrows) - r\n",
    "        array = np.vstack((array, np.zeros((padding, h))))\n",
    "        r, h = array.shape\n",
    "    if h % ncols != 0:\n",
    "        padding = (math.ceil(h / ncols) * ncols) - h\n",
    "        array = np.hstack((array, np.zeros((r, padding))))\n",
    "        r, h = array.shape\n",
    "#     print(array.shape)\n",
    "    num_x_blocks = math.ceil(r / float(nrows))\n",
    "    num_y_blocks = math.ceil(h / float(ncols))\n",
    "    \n",
    "    rows = np.vsplit(array, num_x_blocks)\n",
    "    return [np.array(np.hsplit(row, num_y_blocks)) for row in rows]  \n",
    "#     chunks = array.reshape(h//nrows, nrows, -1, ncols).swapaxes(1, 2).reshape(-1, nrows, ncols)\n",
    "#     return np.split(chunks, num_x_blocks)\n",
    "\n",
    "def gather_blocks_to_pages(splits, num_elem_per_page):\n",
    "    blocks = np.concatenate(splits)\n",
    "    pages = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(blocks):\n",
    "        count = 0\n",
    "        page = []\n",
    "        while i < len(blocks) and count + blocks[i].shape[0] * blocks[i].shape[1] <= num_elem_per_page:\n",
    "            page.append(blocks[i])\n",
    "            count += blocks[i].shape[0] * blocks[i].shape[1]\n",
    "            i += 1\n",
    "        pages.append(page)\n",
    "        print(\"Adding {} elems to page {}\".format(count, len(pages) - 1))\n",
    "    return pages\n",
    "\n",
    "# So we assume that if one page as lesser blocks than the other,\n",
    "# We should see if the smaller one matches the bigger one from\n",
    "# the start, and not anywhere in between\n",
    "def page_similarity(ps1, ps2):\n",
    "    sim = np.zeros((len(ps1), len(ps2)))\n",
    "    \n",
    "    for i, p1 in enumerate(ps1):\n",
    "        for j, p2 in enumerate(ps2):\n",
    "            k = min(len(p1), len(p2))\n",
    "            a = np.array(p1[:k])\n",
    "            b = np.array(p2[:k])\n",
    "            c = np.count_nonzero(np.absolute(a - b) <= 0.01)\n",
    "            sim[i][j] = c / a.size\n",
    "    \n",
    "    return sim\n",
    "            \n",
    "\n",
    "def merge_blocks(blocks, num_blocks_x, num_blocks_y, x, y):\n",
    "    b_x, b_y = blocks[0].shape\n",
    "    t_x, t_y = (b_x * num_blocks_x, b_y * num_blocks_y,)\n",
    "    rows = [np.hstack(blocks[i*num_blocks_y:i*num_blocks_y+num_blocks_y]) for i in range(num_blocks_x)]\n",
    "    matrix = np.vstack(rows)\n",
    "    assert matrix.shape[0] == t_x\n",
    "    r_x = t_x - x\n",
    "    r_y = t_y - y\n",
    "    if r_x == 0 and r_y == 0:\n",
    "        return matrix\n",
    "    elif r_x == 0:\n",
    "        return matrix[:,:-r_y]\n",
    "    elif r_y == 0:\n",
    "        return matrix[:-r_x,:]\n",
    "    else:\n",
    "        return matrix[:-r_x, :-r_y]\n",
    "\n",
    "# merge_blocks(x, 2, 2, 4, 4)\n",
    "# x[0]\n",
    "\n",
    "def pages_to_blocks(pages):\n",
    "    blocks = []\n",
    "    for p in pages:\n",
    "        blocks.extend(p)\n",
    "    return blocks\n",
    "\n",
    "def merge_pages(p1, p2):\n",
    "    ps = []\n",
    "    for i, p in enumerate(p1):\n",
    "        if i >= len(p2):\n",
    "            ps.append(np.array(p))\n",
    "        else:\n",
    "            ps.append((p + p2[i]) / 2)\n",
    "    return ps\n",
    "\n",
    "def combine_similar_pages(ps1, ps2, sim_scores, threshold=0.9):\n",
    "    new_ps1 = [None] * len(ps1)\n",
    "    new_ps2 = [None] * len(ps2)\n",
    "    \n",
    "    for ps1_idx, scores in enumerate(sim_scores):\n",
    "        if np.max(scores) >= threshold:\n",
    "            ps2_idx = np.argmax(scores)\n",
    "            print(\"PS1: Merging {} and {}\".format(ps1_idx, ps2_idx))\n",
    "            new_ps1[ps1_idx] = merge_pages(ps1[ps1_idx], ps2[ps2_idx])\n",
    "            new_ps2[ps2_idx] = ps1_idx\n",
    "        else:\n",
    "            # No need to make new copies here since we should not be using ps1 or ps2 anymore\n",
    "            new_ps1[ps1_idx] = ps1[ps1_idx]\n",
    "\n",
    "    for ps2_idx in range(len(ps2)):\n",
    "        if new_ps2[ps2_idx] is not None:\n",
    "            ps1_idx = new_ps2[ps2_idx]\n",
    "            print(\"PS2: Merging {} and {}\".format(ps2_idx, ps1_idx))\n",
    "            new_ps2[ps2_idx] = merge_pages(ps2[ps2_idx], ps1[ps1_idx])\n",
    "        else:\n",
    "            # No need to make new copies here since we should not be using ps1 or ps2 anymore\n",
    "            new_ps2[ps2_idx] = ps2[ps2_idx]\n",
    "\n",
    "    return new_ps1, new_ps2\n",
    "\n",
    "def share_weights(w1, w2, a, b, c, t):\n",
    "    m, n = w1.shape\n",
    "    x = split(w1, a, b)\n",
    "    bx, by = len(x), x[0].shape[0]\n",
    "\n",
    "    y = split(w2, a, b)\n",
    "    \n",
    "    ps1 = gather_blocks_to_pages(x, c) # ~1MB 16 bytes * 63725\n",
    "    ps2 = gather_blocks_to_pages(y, c) # ~1MB 16 bytes * 63725\n",
    "\n",
    "    sim = page_similarity(ps1, ps2)\n",
    "    \n",
    "    nps1, nps2 = combine_similar_pages(ps1, ps2, sim, t)\n",
    "    \n",
    "    wb1 = pages_to_blocks(nps1)\n",
    "    wb1 = merge_blocks(wb1, bx, by, m, n)\n",
    "    \n",
    "    wb2 = pages_to_blocks(nps2)\n",
    "    wb2 = merge_blocks(wb2, bx, by, m, n)\n",
    "    \n",
    "    return wb1, wb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 63725 elems to page 0\n",
      "Adding 63725 elems to page 1\n",
      "Adding 63725 elems to page 2\n",
      "Adding 63725 elems to page 3\n",
      "Adding 63725 elems to page 4\n",
      "Adding 63725 elems to page 5\n",
      "Adding 10150 elems to page 6\n",
      "Adding 63725 elems to page 0\n",
      "Adding 63725 elems to page 1\n",
      "Adding 63725 elems to page 2\n",
      "Adding 63725 elems to page 3\n",
      "Adding 63725 elems to page 4\n",
      "Adding 63725 elems to page 5\n",
      "Adding 10150 elems to page 6\n",
      "PS1: Merging 1 and 1\n",
      "PS1: Merging 4 and 4\n",
      "PS1: Merging 5 and 5\n",
      "PS2: Merging 1 and 1\n",
      "PS2: Merging 4 and 4\n",
      "PS2: Merging 5 and 5\n"
     ]
    }
   ],
   "source": [
    "model1 = keras.models.load_model('models/model_ff_0_25_2.h5')\n",
    "model2 = keras.models.load_model('models/model_ff_1_25_2.h5')\n",
    "\n",
    "w1, b1 = model1.layers[0].get_weights()\n",
    "w2, b2 = model2.layers[0].get_weights()\n",
    "\n",
    "w1, w2 = share_weights(w1, w2, 5, 5, 63725, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 500)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5438 - accuracy: 0.8119\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5403 - accuracy: 0.8125\n",
      "\n",
      "Test Accuracy:0.8119000196456909\n",
      "Train Accuracy:0.8125166893005371\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.4841 - accuracy: 0.8299\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4730 - accuracy: 0.8318\n",
      "\n",
      "Test Accuracy:0.8299000263214111\n",
      "Train Accuracy:0.8318166732788086\n"
     ]
    }
   ],
   "source": [
    "model1.layers[0].set_weights([w1, b1])\n",
    "model2.layers[0].set_weights([w2, b2])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_and_metrics = model1.evaluate(test_input.to_numpy(), test_label)\n",
    "train_loss_and_metrics = model1.evaluate(train_input.to_numpy(), train_label)\n",
    "print(\"\")\n",
    "print(\"Test Accuracy:\" + str(test_loss_and_metrics[1]))\n",
    "print(\"Train Accuracy:\" + str(train_loss_and_metrics[1]))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_and_metrics = model2.evaluate(test_input.to_numpy(), test_label)\n",
    "train_loss_and_metrics = model2.evaluate(train_input.to_numpy(), train_label)\n",
    "print(\"\")\n",
    "print(\"Test Accuracy:\" + str(test_loss_and_metrics[1]))\n",
    "print(\"Train Accuracy:\" + str(train_loss_and_metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "def fisher(model, raw_data):\n",
    "    y = model.output\n",
    "\n",
    "    row_idx = tf.range(tf.shape(y)[0])\n",
    "    col_idx = tf.argmax(y, axis=1, output_type=tf.dtypes.int32)\n",
    "    full_indices = tf.stack([row_idx, col_idx], axis=1)\n",
    "    fx_tensors = tf.gather_nd(y, full_indices)\n",
    "\n",
    "    x_tensors = model.trainable_weights\n",
    "\n",
    "    num_samples = 100\n",
    "    m = Model(inputs=model.input, outputs=fx_tensors)\n",
    "\n",
    "    fisher_information = []\n",
    "    for v in range(len(x_tensors)):\n",
    "        fisher_information.append(np.zeros(x_tensors[v].get_shape().as_list()).astype(np.float32))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        data_idx = np.random.randint(raw_data.shape[0])\n",
    "        sampled_data = raw_data[data_idx:data_idx+1]\n",
    "        sampled_input_variables = [ sampled_data ]\n",
    "        print ('sample num: %4d, data_idx: %5d' % (i, data_idx))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            p = m(sampled_data)\n",
    "            lo = tf.math.log(p)\n",
    "\n",
    "        gradients = tape.gradient(lo, x_tensors)\n",
    "        derivatives = [g.numpy() for g in gradients]\n",
    "        prob = p.numpy()[0]\n",
    "\n",
    "    #     derivatives, prob = sess.run([tf.gradients(tf.log(fx_tensors), x_tensors), fx_tensors],\n",
    "    #     feed_dict={t: v for t,v in zip(input_tensors, sampled_input_variables)})\n",
    "\n",
    "        for v in range(len(fisher_information)):\n",
    "            fisher_information[v] += np.square(derivatives[v]) * prob\n",
    "\n",
    "    for v in range(len(fisher_information)):\n",
    "        fisher_information[v] /= num_samples\n",
    "    \n",
    "    return fisher_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample num:    0, data_idx:  1815\n",
      "sample num:    1, data_idx: 30158\n",
      "sample num:    2, data_idx:   605\n",
      "sample num:    3, data_idx: 35390\n",
      "sample num:    4, data_idx:  4884\n",
      "sample num:    5, data_idx: 20392\n",
      "sample num:    6, data_idx: 31380\n",
      "sample num:    7, data_idx: 29840\n",
      "sample num:    8, data_idx: 16918\n",
      "sample num:    9, data_idx: 18693\n",
      "sample num:   10, data_idx:  2447\n",
      "sample num:   11, data_idx: 20680\n",
      "sample num:   12, data_idx:  2298\n",
      "sample num:   13, data_idx: 25015\n",
      "sample num:   14, data_idx:  8542\n",
      "sample num:   15, data_idx: 36711\n",
      "sample num:   16, data_idx: 11445\n",
      "sample num:   17, data_idx: 11763\n",
      "sample num:   18, data_idx: 13824\n",
      "sample num:   19, data_idx: 26055\n",
      "sample num:   20, data_idx:  5715\n",
      "sample num:   21, data_idx:    40\n",
      "sample num:   22, data_idx:  3707\n",
      "sample num:   23, data_idx:  7032\n",
      "sample num:   24, data_idx: 33471\n",
      "sample num:   25, data_idx: 27226\n",
      "sample num:   26, data_idx:  6420\n",
      "sample num:   27, data_idx: 27902\n",
      "sample num:   28, data_idx: 18783\n",
      "sample num:   29, data_idx: 23302\n",
      "sample num:   30, data_idx: 32782\n",
      "sample num:   31, data_idx:  7457\n",
      "sample num:   32, data_idx: 10996\n",
      "sample num:   33, data_idx: 29862\n",
      "sample num:   34, data_idx:  4223\n",
      "sample num:   35, data_idx: 22936\n",
      "sample num:   36, data_idx: 13172\n",
      "sample num:   37, data_idx: 34477\n",
      "sample num:   38, data_idx: 25996\n",
      "sample num:   39, data_idx: 17475\n",
      "sample num:   40, data_idx: 28828\n",
      "sample num:   41, data_idx: 13214\n",
      "sample num:   42, data_idx: 11912\n",
      "sample num:   43, data_idx: 31106\n",
      "sample num:   44, data_idx: 27919\n",
      "sample num:   45, data_idx:  4456\n",
      "sample num:   46, data_idx:  2492\n",
      "sample num:   47, data_idx: 36024\n",
      "sample num:   48, data_idx: 35806\n",
      "sample num:   49, data_idx: 27170\n",
      "sample num:   50, data_idx:  4411\n",
      "sample num:   51, data_idx: 12538\n",
      "sample num:   52, data_idx: 34391\n",
      "sample num:   53, data_idx: 10214\n",
      "sample num:   54, data_idx:  7405\n",
      "sample num:   55, data_idx: 29752\n",
      "sample num:   56, data_idx: 24940\n",
      "sample num:   57, data_idx: 26987\n",
      "sample num:   58, data_idx: 25604\n",
      "sample num:   59, data_idx: 18668\n",
      "sample num:   60, data_idx: 17316\n",
      "sample num:   61, data_idx:  6079\n",
      "sample num:   62, data_idx: 33974\n",
      "sample num:   63, data_idx:  4336\n",
      "sample num:   64, data_idx:  8529\n",
      "sample num:   65, data_idx: 31983\n",
      "sample num:   66, data_idx:  8731\n",
      "sample num:   67, data_idx:  3339\n",
      "sample num:   68, data_idx: 15109\n",
      "sample num:   69, data_idx: 36604\n",
      "sample num:   70, data_idx:  2805\n",
      "sample num:   71, data_idx:  7902\n",
      "sample num:   72, data_idx:  7987\n",
      "sample num:   73, data_idx: 22905\n",
      "sample num:   74, data_idx: 16746\n",
      "sample num:   75, data_idx: 25223\n",
      "sample num:   76, data_idx: 27836\n",
      "sample num:   77, data_idx: 30756\n",
      "sample num:   78, data_idx: 27996\n",
      "sample num:   79, data_idx: 18237\n",
      "sample num:   80, data_idx: 21451\n",
      "sample num:   81, data_idx:  7263\n",
      "sample num:   82, data_idx:   162\n",
      "sample num:   83, data_idx: 21696\n",
      "sample num:   84, data_idx: 22828\n",
      "sample num:   85, data_idx:  7888\n",
      "sample num:   86, data_idx: 12262\n",
      "sample num:   87, data_idx: 32949\n",
      "sample num:   88, data_idx: 17829\n",
      "sample num:   89, data_idx:  7517\n",
      "sample num:   90, data_idx:  1809\n",
      "sample num:   91, data_idx: 27448\n",
      "sample num:   92, data_idx: 22012\n",
      "sample num:   93, data_idx:  5297\n",
      "sample num:   94, data_idx:  1851\n",
      "sample num:   95, data_idx:  5635\n",
      "sample num:   96, data_idx: 20046\n",
      "sample num:   97, data_idx: 27901\n",
      "sample num:   98, data_idx: 25482\n",
      "sample num:   99, data_idx: 10655\n"
     ]
    }
   ],
   "source": [
    "model1 = keras.models.load_model('models/model_ff_0_25_2.h5')\n",
    "model2 = keras.models.load_model('models/model_ff_1_25_2.h5')\n",
    "\n",
    "model_train_data = np.vstack((shared_train_data, private_train_data[0]))\n",
    "model_train_labels = np.concatenate(\n",
    "    (shared_train_labels, private_train_labels[0]))\n",
    "\n",
    "fi = fisher(model1, model_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# if using a Jupyter notebook, include:\n",
    "%matplotlib inline\n",
    "\n",
    "d = [x.flatten() for x in fi]\n",
    "d = np.concatenate(d)\n",
    "\n",
    "# d = a[0].flatten()\n",
    "# index = np.arange(len(d))\n",
    "# plt.bar(index, d)\n",
    "plt.plot(d)\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
