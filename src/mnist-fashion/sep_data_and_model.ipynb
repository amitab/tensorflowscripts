{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means:\n",
      "pixel1    0.000900\n",
      "pixel2    0.006150\n",
      "pixel3    0.035333\n",
      "pixel4    0.101933\n",
      "pixel5    0.247967\n",
      "dtype: float64\n",
      "pixel1    0.000867\n",
      "pixel2    0.007167\n",
      "pixel3    0.031600\n",
      "pixel4    0.109067\n",
      "pixel5    0.265300\n",
      "dtype: float64\n",
      "pixel1    0.000933\n",
      "pixel2    0.005133\n",
      "pixel3    0.039067\n",
      "pixel4    0.094800\n",
      "pixel5    0.230633\n",
      "dtype: float64\n",
      "Stds:\n",
      "pixel1    0.094689\n",
      "pixel2    0.271011\n",
      "pixel3    1.222324\n",
      "pixel4    2.452871\n",
      "pixel5    4.306912\n",
      "dtype: float64\n",
      "pixel1    0.084850\n",
      "pixel2    0.348022\n",
      "pixel3    1.588842\n",
      "pixel4    2.963961\n",
      "pixel5    5.029310\n",
      "dtype: float64\n",
      "pixel1    0.103599\n",
      "pixel2    0.160544\n",
      "pixel3    0.680996\n",
      "pixel4    1.802273\n",
      "pixel5    3.435846\n",
      "dtype: float64\n",
      "                  0             1             2             3             4  \\\n",
      "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
      "mean       0.100000      0.100000      0.100000      0.100000      0.100000   \n",
      "std        0.300024      0.300024      0.300024      0.300025      0.300024   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "                  5             6             7             8             9  \n",
      "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000  \n",
      "mean       0.100000      0.100000      0.100000      0.100000      0.100000  \n",
      "std        0.300024      0.300024      0.300024      0.300024      0.300024  \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000  \n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/kemalty/feed-forward-n-n-with-keras-for-fashion-mnist/notebook?select=fashion-mnist_train.csv\n",
    "\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Loading and pre-processing training dataset\n",
    "train_data = pd.read_csv(\"data/fashion-mnist_train.csv\")\n",
    "train_data_a = train_data.loc[train_data.label>=5]\n",
    "train_data_b = train_data.loc[train_data.label<5]\n",
    "\n",
    "train_label = pd.DataFrame(train_data[[\"label\"]].copy(deep=False)) # Seperate labels (y) from inputs (X)\n",
    "train_label_a = pd.DataFrame(train_data_a[[\"label\"]].copy(deep=False)) - 5\n",
    "train_label_b = pd.DataFrame(train_data_b[[\"label\"]].copy(deep=False))\n",
    "\n",
    "train_input = pd.DataFrame(train_data.drop(\"label\", 1, inplace=False))\n",
    "train_input_a = pd.DataFrame(train_data_a.drop(\"label\", 1, inplace=False))\n",
    "train_input_b = pd.DataFrame(train_data_b.drop(\"label\", 1, inplace=False))\n",
    "del train_data\n",
    "del train_data_a\n",
    "del train_data_b\n",
    "\n",
    "# Convert labels to dummies (one-hot encoding) so that they can be used in the output layer\n",
    "train_label = to_categorical(train_label)\n",
    "train_label_a = to_categorical(train_label_a)\n",
    "train_label_b = to_categorical(train_label_b)\n",
    "\n",
    "# Normalize the inputs\n",
    "train_means = train_input.mean(axis=0) # Keep these for test too\n",
    "train_means_a = train_input_a.mean(axis=0) # Keep these for test too\n",
    "train_means_b = train_input_b.mean(axis=0) # Keep these for test too\n",
    "\n",
    "train_stds  = train_input.std(axis=0)\n",
    "train_stds_a  = train_input_a.std(axis=0)\n",
    "train_stds_b  = train_input_b.std(axis=0)\n",
    "\n",
    "print(\"Means:\")\n",
    "print(train_means.head(5))\n",
    "print(train_means_a.head(5))\n",
    "print(train_means_b.head(5))\n",
    "print(\"Stds:\")\n",
    "print(train_stds.head(5))\n",
    "print(train_stds_a.head(5))\n",
    "print(train_stds_b.head(5))\n",
    "\n",
    "train_input = train_input - train_means # Zero mean\n",
    "train_input_a = train_input_a - train_means_a # Zero mean\n",
    "train_input_b = train_input_b - train_means_b # Zero mean\n",
    "\n",
    "train_input = train_input / train_stds # 1 standard deviation\n",
    "train_input_a = train_input_a / train_stds_a # 1 standard deviation\n",
    "train_input_b = train_input_b / train_stds_b # 1 standard deviation\n",
    "\n",
    "# Loading and pre-processing testing dataset\n",
    "test_data = pd.read_csv(\"data/fashion-mnist_test.csv\") # Load the csv from file\n",
    "test_data_a = test_data.loc[test_data.label>=5]\n",
    "test_data_b = test_data.loc[test_data.label<5]\n",
    "\n",
    "test_label = pd.DataFrame(test_data[[\"label\"]].copy(deep=False)) # Seperate labels (y) from inputs (X)\n",
    "test_label_a = pd.DataFrame(test_data_a[[\"label\"]].copy(deep=False)) - 5 # Seperate labels (y) from inputs (X)\n",
    "test_label_b = pd.DataFrame(test_data_b[[\"label\"]].copy(deep=False)) # Seperate labels (y) from inputs (X)\n",
    "\n",
    "test_input = pd.DataFrame(test_data.drop(\"label\", 1, inplace=False))\n",
    "test_input_a = pd.DataFrame(test_data_a.drop(\"label\", 1, inplace=False))\n",
    "test_input_b = pd.DataFrame(test_data_b.drop(\"label\", 1, inplace=False))\n",
    "\n",
    "del test_data\n",
    "del test_data_a\n",
    "del test_data_b\n",
    "\n",
    "# Convert labels to dummies (one-hot encoding) so that they can be used in the output layer\n",
    "test_label = to_categorical(test_label)\n",
    "test_label_a = to_categorical(test_label_a)\n",
    "test_label_b = to_categorical(test_label_b)\n",
    "\n",
    "print(pd.DataFrame(test_label).describe())\n",
    "\n",
    "# Apply normalization\n",
    "test_input = test_input - train_means # Zero mean\n",
    "test_input = test_input / train_stds # 1 standard deviation\n",
    "test_input_a = test_input_a - train_means_a # Zero mean\n",
    "test_input_a = test_input_a / train_stds_a # 1 standard deviation\n",
    "test_input_b = test_input_b - train_means_b # Zero mean\n",
    "test_input_b = test_input_b / train_stds_b # 1 standard deviation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X_train = train_input.to_numpy()\n",
    "y_train = train_label\n",
    "\n",
    "X_train_d = [train_input_a.to_numpy(), train_input_b.to_numpy()]\n",
    "y_train_d = [train_label_a, train_label_b]\n",
    "\n",
    "X_test_d = [test_input_a.to_numpy(), test_input_b.to_numpy()]\n",
    "y_test_d = [test_label_a, test_label_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               150300    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 130       \n",
      "=================================================================\n",
      "Total params: 629,555\n",
      "Trainable params: 629,555\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 1.4140 - accuracy: 0.3744\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 1.0720 - accuracy: 0.5275\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.7918 - accuracy: 0.6686\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.5823 - accuracy: 0.7964\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.4500 - accuracy: 0.8626\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.3121 - accuracy: 0.9060\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.3004 - accuracy: 0.9061\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.2734 - accuracy: 0.9145\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.2913 - accuracy: 0.9070\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 0.2337 - accuracy: 0.9285\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.2408 - accuracy: 0.9261\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.2257 - accuracy: 0.9322\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.2404 - accuracy: 0.9275\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 0.1948 - accuracy: 0.9416\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.2209 - accuracy: 0.9322\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.1731 - accuracy: 0.9467\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.1796 - accuracy: 0.9451\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.2608 - accuracy: 0.9195\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.1607 - accuracy: 0.9514\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.1702 - accuracy: 0.9489\n",
      "157/157 [==============================] - 0s 1ms/step - loss: 0.1552 - accuracy: 0.9486\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1197 - accuracy: 0.9625\n",
      "\n",
      "Test Accuracy:0.9485999941825867\n",
      "Train Accuracy:0.9624999761581421\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Set-up the network\n",
    "model = Sequential()\n",
    "model.add(Dense(units=500, input_dim=train_input.shape[1],\n",
    "                activation=\"relu\",\n",
    "                 kernel_initializer=\"random_uniform\",\n",
    "                 bias_initializer=\"zeros\"))\n",
    "model.add(Dropout(0.30))\n",
    "model.add(Dense(units=300, activation=\"relu\", kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units=200, activation=\"relu\", kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\"))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\"))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(units=50, activation=\"relu\", kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\"))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(units=25, activation=\"relu\", kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(units=5, activation=\"softmax\"))\n",
    "\n",
    "# Print out the network configuration\n",
    "print(model.summary())\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "import keras\n",
    "\n",
    "model_train_data = X_train_d[0]\n",
    "model_train_labels = y_train_d[0]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "          optimizer=\"RMSprop\",#lr=0.0001),\n",
    "          metrics=['accuracy'])\n",
    "model.fit(model_train_data, model_train_labels, epochs=20, batch_size=6000)\n",
    "\n",
    "model_name = \"models/model_ff_sep_dense.h5\"\n",
    "model.save(model_name)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_and_metrics = model.evaluate(X_test_d[0], y_test_d[0])\n",
    "train_loss_and_metrics = model.evaluate(model_train_data, model_train_labels)\n",
    "print(\"\")\n",
    "print(\"Test Accuracy:\" + str(test_loss_and_metrics[1]))\n",
    "print(\"Train Accuracy:\" + str(train_loss_and_metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_ROWS = 28\n",
    "IMG_COLS = 28\n",
    "NUM_CLASSES = 5\n",
    "NO_EPOCHS = 50\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               147584    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 240,901\n",
      "Trainable params: 240,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "235/235 [==============================] - 8s 34ms/step - loss: 0.3927 - accuracy: 0.8590\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 8s 36ms/step - loss: 0.2430 - accuracy: 0.9139\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 0.1994 - accuracy: 0.9285\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 0.1736 - accuracy: 0.9385\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 0.1514 - accuracy: 0.9448\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 0.1304 - accuracy: 0.9532\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 0.1123 - accuracy: 0.9594\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 0.0977 - accuracy: 0.9638\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 0.0801 - accuracy: 0.9716\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 0.0690 - accuracy: 0.9746\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 0.0619 - accuracy: 0.9777\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 0.0537 - accuracy: 0.9800\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 0.0511 - accuracy: 0.9809\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 9s 39ms/step - loss: 0.0443 - accuracy: 0.9834\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 0.0345 - accuracy: 0.9871\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 0.0359 - accuracy: 0.9868\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 0.0286 - accuracy: 0.9903\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 0.0276 - accuracy: 0.9897\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 9s 40ms/step - loss: 0.0289 - accuracy: 0.9901\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 10s 41ms/step - loss: 0.0305 - accuracy: 0.9892\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 10s 41ms/step - loss: 0.0245 - accuracy: 0.9912\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 10s 41ms/step - loss: 0.0266 - accuracy: 0.9897\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.0165 - accuracy: 0.9940\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 10s 41ms/step - loss: 0.0230 - accuracy: 0.9916\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.0258 - accuracy: 0.9907\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.0149 - accuracy: 0.9950\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.0103 - accuracy: 0.9962\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.0184 - accuracy: 0.9937\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.0184 - accuracy: 0.9941\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.0173 - accuracy: 0.9943\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.0202 - accuracy: 0.9937\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 10s 44ms/step - loss: 0.0201 - accuracy: 0.9927\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.0154 - accuracy: 0.9947\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 10s 44ms/step - loss: 0.0098 - accuracy: 0.9964\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 10s 44ms/step - loss: 0.0165 - accuracy: 0.9946\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.0198 - accuracy: 0.9933\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 10s 44ms/step - loss: 0.0106 - accuracy: 0.9963\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 10s 44ms/step - loss: 0.0128 - accuracy: 0.9958\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 10s 44ms/step - loss: 0.0115 - accuracy: 0.9958\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 10s 44ms/step - loss: 0.0160 - accuracy: 0.9944\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 11s 45ms/step - loss: 0.0116 - accuracy: 0.9960\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 10s 44ms/step - loss: 0.0152 - accuracy: 0.9951\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 10s 45ms/step - loss: 0.0078 - accuracy: 0.9976\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 10s 45ms/step - loss: 0.0136 - accuracy: 0.9956\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 11s 45ms/step - loss: 0.0056 - accuracy: 0.9981\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 11s 46ms/step - loss: 0.0015 - accuracy: 0.9995\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 10s 45ms/step - loss: 0.0149 - accuracy: 0.9955\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 11s 45ms/step - loss: 0.0216 - accuracy: 0.9930\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 11s 45ms/step - loss: 0.0155 - accuracy: 0.9948\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 11s 45ms/step - loss: 0.0072 - accuracy: 0.9977\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 0.4896 - accuracy: 0.9358\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.0089 - accuracy: 0.9973\n",
      "\n",
      "Test Accuracy:0.9358000159263611\n",
      "Train Accuracy:0.9973333477973938\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D\n",
    "\n",
    "# Model\n",
    "model1 = Sequential()\n",
    "# Add convolution 2D\n",
    "model1.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 input_shape=(IMG_ROWS, IMG_COLS, 1)))\n",
    "model1.add(MaxPooling2D((2, 2)))\n",
    "model1.add(Conv2D(64, \n",
    "                 kernel_size=(3, 3), \n",
    "                 activation='relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "model1.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "\n",
    "model1.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "len_train_data = len(X_train_d[1])\n",
    "len_test_data = len(X_test_d[1])\n",
    "\n",
    "model_train_data = X_train_d[1].reshape(len_train_data, IMG_ROWS, IMG_COLS, 1)\n",
    "model_train_labels = y_train_d[1]\n",
    "\n",
    "model_test_data = X_test_d[1].reshape(len_test_data, IMG_ROWS, IMG_COLS, 1)\n",
    "model_test_labels = y_test_d[1]\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "model1.fit(model_train_data, model_train_labels,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  epochs=NO_EPOCHS,\n",
    "                  verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_and_metrics = model1.evaluate(model_test_data, model_test_labels)\n",
    "train_loss_and_metrics = model1.evaluate(model_train_data, model_train_labels)\n",
    "print(\"\")\n",
    "print(\"Test Accuracy:\" + str(test_loss_and_metrics[1]))\n",
    "print(\"Train Accuracy:\" + str(train_loss_and_metrics[1]))\n",
    "\n",
    "model_name = \"models/model_ff_sep_conv.h5\"\n",
    "model1.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def split(array, nrows, ncols):\n",
    "    \"\"\"Split a matrix into sub-matrices.\"\"\"\n",
    "    r, h = array.shape\n",
    "    if r % nrows != 0:\n",
    "        padding = (math.ceil(r / nrows) * nrows) - r\n",
    "        array = np.vstack((array, np.zeros((padding, h))))\n",
    "        r, h = array.shape\n",
    "    if h % ncols != 0:\n",
    "        padding = (math.ceil(h / ncols) * ncols) - h\n",
    "        array = np.hstack((array, np.zeros((r, padding))))\n",
    "        r, h = array.shape\n",
    "#     print(array.shape)\n",
    "    num_x_blocks = math.ceil(r / float(nrows))\n",
    "    num_y_blocks = math.ceil(h / float(ncols))\n",
    "    \n",
    "    rows = np.vsplit(array, num_x_blocks)\n",
    "    return [np.array(np.hsplit(row, num_y_blocks)) for row in rows]  \n",
    "#     chunks = array.reshape(h//nrows, nrows, -1, ncols).swapaxes(1, 2).reshape(-1, nrows, ncols)\n",
    "#     return np.split(chunks, num_x_blocks)\n",
    "\n",
    "def gather_blocks_to_pages(splits, num_elem_per_page):\n",
    "    blocks = np.concatenate(splits)\n",
    "    pages = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(blocks):\n",
    "        count = 0\n",
    "        page = []\n",
    "        while i < len(blocks) and count + blocks[i].shape[0] * blocks[i].shape[1] <= num_elem_per_page:\n",
    "            page.append(blocks[i])\n",
    "            count += blocks[i].shape[0] * blocks[i].shape[1]\n",
    "            i += 1\n",
    "        pages.append(page)\n",
    "        print(\"Adding {} elems to page {}\".format(count, len(pages) - 1))\n",
    "    return pages\n",
    "\n",
    "# So we assume that if one page as lesser blocks than the other,\n",
    "# We should see if the smaller one matches the bigger one from\n",
    "# the start, and not anywhere in between\n",
    "def page_similarity(ps1, ps2):\n",
    "    sim = np.zeros((len(ps1), len(ps2)))\n",
    "    \n",
    "    for i, p1 in enumerate(ps1):\n",
    "        for j, p2 in enumerate(ps2):\n",
    "            k = min(len(p1), len(p2))\n",
    "            a = np.array(p1[:k])\n",
    "            b = np.array(p2[:k])\n",
    "            c = np.count_nonzero(np.absolute(a - b) <= 0.01)\n",
    "            sim[i][j] = c / a.size\n",
    "    \n",
    "    return sim\n",
    "            \n",
    "\n",
    "def merge_blocks(blocks, num_blocks_x, num_blocks_y, x, y):\n",
    "    b_x, b_y = blocks[0].shape\n",
    "    t_x, t_y = (b_x * num_blocks_x, b_y * num_blocks_y,)\n",
    "    rows = [np.hstack(blocks[i*num_blocks_y:i*num_blocks_y+num_blocks_y]) for i in range(num_blocks_x)]\n",
    "    matrix = np.vstack(rows)\n",
    "    assert matrix.shape[0] == t_x\n",
    "    r_x = t_x - x\n",
    "    r_y = t_y - y\n",
    "    if r_x == 0 and r_y == 0:\n",
    "        return matrix\n",
    "    elif r_x == 0:\n",
    "        return matrix[:,:-r_y]\n",
    "    elif r_y == 0:\n",
    "        return matrix[:-r_x,:]\n",
    "    else:\n",
    "        return matrix[:-r_x, :-r_y]\n",
    "\n",
    "# merge_blocks(x, 2, 2, 4, 4)\n",
    "# x[0]\n",
    "\n",
    "def pages_to_blocks(pages):\n",
    "    blocks = []\n",
    "    for p in pages:\n",
    "        blocks.extend(p)\n",
    "    return blocks\n",
    "\n",
    "def merge_pages(p1, p2):\n",
    "#     print(p1)\n",
    "#     print(p2)\n",
    "    ps = []\n",
    "    for i, p in enumerate(p1):\n",
    "        if i >= len(p2):\n",
    "            ps.append(np.array(p))\n",
    "        else:\n",
    "            ps.append((p + p2[i]) / 2)\n",
    "    return ps\n",
    "\n",
    "def combine_similar_pages(ps1, ps2, sim_scores, threshold=0.9):\n",
    "    new_ps1 = [None] * len(ps1)\n",
    "    new_ps2 = [None] * len(ps2)\n",
    "    \n",
    "    for ps1_idx, scores in enumerate(sim_scores):\n",
    "        if np.max(scores) >= threshold:\n",
    "            ps2_idx = np.argmax(scores)\n",
    "            print(\"PS1: Merging {} and {}\".format(ps1_idx, ps2_idx))\n",
    "            new_ps1[ps1_idx] = merge_pages(ps1[ps1_idx], ps2[ps2_idx])\n",
    "            new_ps2[ps2_idx] = ps1_idx\n",
    "        else:\n",
    "            # No need to make new copies here since we should not be using ps1 or ps2 anymore\n",
    "            new_ps1[ps1_idx] = ps1[ps1_idx]\n",
    "\n",
    "    for ps2_idx in range(len(ps2)):\n",
    "        if new_ps2[ps2_idx] is not None:\n",
    "            ps1_idx = new_ps2[ps2_idx]\n",
    "            print(\"PS2: Merging {} and {}\".format(ps2_idx, ps1_idx))\n",
    "            new_ps2[ps2_idx] = merge_pages(ps2[ps2_idx], ps1[ps1_idx])\n",
    "        else:\n",
    "            # No need to make new copies here since we should not be using ps1 or ps2 anymore\n",
    "            new_ps2[ps2_idx] = ps2[ps2_idx]\n",
    "\n",
    "    return new_ps1, new_ps2\n",
    "\n",
    "def weight_sim(w1, w2):\n",
    "    nw1 = w1.flatten()\n",
    "    nw2 = w2.flatten()\n",
    "    if nw1.size < nw2.size:\n",
    "        rem = nw2.size - nw1.size\n",
    "        nw2 = nw2[:-rem]\n",
    "    elif nw1.size > nw2.size:\n",
    "        rem = nw1.size - nw2.size\n",
    "        nw1 = nw1[:-rem]\n",
    "        \n",
    "    return np.count_nonzero(np.absolute(nw2 - nw1) <= 0.01) / nw1.size\n",
    "\n",
    "def share_weights(w1, w2, a, b, c, t):\n",
    "    m, n = w1.shape\n",
    "    x = split(w1, a, b)\n",
    "    bx, by = len(x), x[0].shape[0]\n",
    "\n",
    "    o, p = w2.shape\n",
    "    y = split(w2, a, b)\n",
    "    cx, cy = len(y), y[0].shape[0]\n",
    "    \n",
    "    ps1 = gather_blocks_to_pages(x, c) # ~1MB 16 bytes * 63725\n",
    "    ps2 = gather_blocks_to_pages(y, c) # ~1MB 16 bytes * 63725\n",
    "\n",
    "    sim = page_similarity(ps1, ps2)\n",
    "    print(sim)\n",
    "    \n",
    "    nps1, nps2 = combine_similar_pages(ps1, ps2, sim, t)\n",
    "    \n",
    "    wb1 = pages_to_blocks(nps1)\n",
    "    wb1 = merge_blocks(wb1, bx, by, m, n)\n",
    "    \n",
    "    wb2 = pages_to_blocks(nps2)\n",
    "    wb2 = merge_blocks(wb2, cx, cy, o, p)\n",
    "    \n",
    "    return wb1, wb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "model = keras.models.load_model(\"models/model_ff_sep_dense.h5\")\n",
    "model1 = keras.models.load_model(\"models/model_ff_sep_conv.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 5)\n",
      "(3, 3, 1, 32)\n",
      "Adding 100 elems to page 0\n",
      "Adding 100 elems to page 1\n",
      "Adding 100 elems to page 2\n",
      "Adding 100 elems to page 0\n",
      "Adding 100 elems to page 1\n",
      "Adding 100 elems to page 2\n",
      "Adding 100 elems to page 3\n",
      "[[0.08 0.06 0.06 0.53]\n",
      " [0.09 0.07 0.07 0.53]\n",
      " [0.12 0.1  0.1  0.67]]\n",
      "PS1: Merging 2 and 3\n",
      "PS2: Merging 3 and 2\n"
     ]
    }
   ],
   "source": [
    "w1, b1 = model.layers[12].get_weights()\n",
    "w2, b2 = model1.layers[0].get_weights()\n",
    "print(w1.shape)\n",
    "print(w2.shape)\n",
    "w2 = w2.reshape(9, 32)\n",
    "w1, w2 = share_weights(w1, w2, 10, 10, 128, 0.6) #63725\n",
    "w2 = w2.reshape(3,3,1,32)\n",
    "model.layers[12].set_weights([w1, b1])\n",
    "model1.layers[0].set_weights([w2, b2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 63725 elems to page 0\n",
      "Adding 63725 elems to page 1\n",
      "Adding 22550 elems to page 2\n",
      "Adding 63725 elems to page 0\n",
      "Adding 63725 elems to page 1\n",
      "Adding 22700 elems to page 2\n",
      "[[0.09245979 0.0947195  0.0922467 ]\n",
      " [0.09311887 0.09542566 0.09008811]\n",
      " [0.09432373 0.09827051 0.0902439 ]]\n",
      "PS1: Merging 0 and 1\n",
      "PS1: Merging 1 and 1\n",
      "PS1: Merging 2 and 1\n",
      "PS2: Merging 1 and 2\n"
     ]
    }
   ],
   "source": [
    "w1, b1 = model.layers[2].get_weights()\n",
    "w2, b2 = model1.layers[6].get_weights()\n",
    "w1, w2 = share_weights(w1, w2, 5, 5, 63725, 0.094) #63725\n",
    "model.layers[2].set_weights([w1, b1])\n",
    "model1.layers[6].set_weights([w2, b2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 200)\n",
      "(3, 3, 64, 128)\n",
      "Adding 16375 elems to page 0\n",
      "Adding 16375 elems to page 1\n",
      "Adding 16375 elems to page 2\n",
      "Adding 10875 elems to page 3\n",
      "Adding 16375 elems to page 0\n",
      "Adding 16375 elems to page 1\n",
      "Adding 16375 elems to page 2\n",
      "Adding 16375 elems to page 3\n",
      "Adding 16375 elems to page 4\n",
      "Adding 75 elems to page 5\n",
      "[[0.07859542 0.07358779 0.09196947 0.10112977 0.10283969 0.14666667]\n",
      " [0.08       0.07969466 0.0919084  0.10375573 0.1030229  0.14666667]\n",
      " [0.08207634 0.07859542 0.08940458 0.10222901 0.1000916  0.09333333]\n",
      " [0.08193103 0.07632184 0.08468966 0.10041379 0.09636782 0.06666667]]\n",
      "PS1: Merging 0 and 5\n",
      "PS1: Merging 1 and 5\n",
      "PS1: Merging 2 and 3\n",
      "PS1: Merging 3 and 3\n",
      "PS2: Merging 3 and 3\n",
      "PS2: Merging 5 and 1\n"
     ]
    }
   ],
   "source": [
    "w1, b1 = model.layers[4].get_weights()\n",
    "w2, b2 = model1.layers[4].get_weights()\n",
    "print(w1.shape)\n",
    "print(w2.shape)\n",
    "w2 = w2.reshape(9, 8192)\n",
    "w1, w2 = share_weights(w1, w2, 5, 5, 16384, 0.088) #63725\n",
    "w2 = w2.reshape(3,3,64,128)\n",
    "model.layers[4].set_weights([w1, b1])\n",
    "model1.layers[4].set_weights([w2, b2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 100)\n",
      "(3, 3, 32, 64)\n",
      "Adding 4075 elems to page 0\n",
      "Adding 4075 elems to page 1\n",
      "Adding 4075 elems to page 2\n",
      "Adding 4075 elems to page 3\n",
      "Adding 3700 elems to page 4\n",
      "Adding 4075 elems to page 0\n",
      "Adding 4075 elems to page 1\n",
      "Adding 4075 elems to page 2\n",
      "Adding 4075 elems to page 3\n",
      "Adding 2550 elems to page 4\n",
      "[[0.06699387 0.06969325 0.06576687 0.06576687 0.07215686]\n",
      " [0.06233129 0.07067485 0.06674847 0.07018405 0.0745098 ]\n",
      " [0.07067485 0.06576687 0.06895706 0.07018405 0.07529412]\n",
      " [0.06846626 0.07239264 0.06404908 0.06773006 0.07882353]\n",
      " [0.06378378 0.07       0.06972973 0.06675676 0.07529412]]\n",
      "PS1: Merging 2 and 4\n",
      "PS1: Merging 3 and 4\n",
      "PS1: Merging 4 and 4\n",
      "PS2: Merging 4 and 4\n"
     ]
    }
   ],
   "source": [
    "w1, b1 = model.layers[6].get_weights()\n",
    "w2, b2 = model1.layers[2].get_weights()\n",
    "print(w1.shape)\n",
    "print(w2.shape)\n",
    "w2 = w2.reshape(288, 64)\n",
    "w1, w2 = share_weights(w1, w2, 5, 5, 4096, 0.075) #63725\n",
    "w2 = w2.reshape(3,3,32,64)\n",
    "model.layers[6].set_weights([w1, b1])\n",
    "model1.layers[2].set_weights([w2, b2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 1ms/step - loss: 0.9280 - accuracy: 0.9458\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.9181 - accuracy: 0.9564\n",
      "\n",
      "Test Accuracy:0.9458000063896179\n",
      "Train Accuracy:0.9563666582107544\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 0.4210 - accuracy: 0.9338\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0250 - accuracy: 0.9917\n",
      "\n",
      "Test Accuracy:0.9337999820709229\n",
      "Train Accuracy:0.9916999936103821\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss_and_metrics = model.evaluate(X_test_d[0], y_test_d[0])\n",
    "train_loss_and_metrics = model.evaluate(X_train_d[0], y_train_d[0])\n",
    "print(\"\")\n",
    "print(\"Test Accuracy:\" + str(test_loss_and_metrics[1]))\n",
    "print(\"Train Accuracy:\" + str(train_loss_and_metrics[1]))\n",
    "\n",
    "len_train_data = len(X_train_d[1])\n",
    "len_test_data = len(X_test_d[1])\n",
    "\n",
    "model_train_data = X_train_d[1].reshape(len_train_data, IMG_ROWS, IMG_COLS, 1)\n",
    "model_train_labels = y_train_d[1]\n",
    "\n",
    "model_test_data = X_test_d[1].reshape(len_test_data, IMG_ROWS, IMG_COLS, 1)\n",
    "model_test_labels = y_test_d[1]\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_and_metrics = model1.evaluate(model_test_data, model_test_labels)\n",
    "train_loss_and_metrics = model1.evaluate(model_train_data, model_train_labels)\n",
    "print(\"\")\n",
    "print(\"Test Accuracy:\" + str(test_loss_and_metrics[1]))\n",
    "print(\"Train Accuracy:\" + str(train_loss_and_metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "def fisher(model, raw_data):\n",
    "    y = model.output\n",
    "\n",
    "    row_idx = tf.range(tf.shape(y)[0])\n",
    "    col_idx = tf.argmax(y, axis=1, output_type=tf.dtypes.int32)\n",
    "    full_indices = tf.stack([row_idx, col_idx], axis=1)\n",
    "    fx_tensors = tf.gather_nd(y, full_indices)\n",
    "\n",
    "    x_tensors = model.trainable_weights\n",
    "\n",
    "    num_samples = 1000\n",
    "    m = Model(inputs=model.input, outputs=fx_tensors)\n",
    "\n",
    "    fisher_information = []\n",
    "    for v in range(len(x_tensors)):\n",
    "        fisher_information.append(np.zeros(x_tensors[v].get_shape().as_list()).astype(np.float32))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        data_idx = np.random.randint(raw_data.shape[0])\n",
    "        sampled_data = raw_data[data_idx:data_idx+1]\n",
    "        sampled_input_variables = [ sampled_data ]\n",
    "#         print ('sample num: %4d, data_idx: %5d' % (i, data_idx))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            p = m(sampled_data)\n",
    "            lo = tf.math.log(p)\n",
    "\n",
    "        gradients = tape.gradient(lo, x_tensors)\n",
    "        derivatives = [g.numpy() for g in gradients]\n",
    "        prob = p.numpy()[0]\n",
    "\n",
    "    #     derivatives, prob = sess.run([tf.gradients(tf.log(fx_tensors), x_tensors), fx_tensors],\n",
    "    #     feed_dict={t: v for t,v in zip(input_tensors, sampled_input_variables)})\n",
    "\n",
    "        for v in range(len(fisher_information)):\n",
    "            fisher_information[v] += np.square(derivatives[v]) * prob\n",
    "\n",
    "    for v in range(len(fisher_information)):\n",
    "        fisher_information[v] /= num_samples\n",
    "    \n",
    "    return fisher_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "model = keras.models.load_model(\"models/model_ff_sep_dense.h5\")\n",
    "model1 = keras.models.load_model(\"models/model_ff_sep_conv.h5\")\n",
    "\n",
    "model_train_data = X_train_d[0]\n",
    "model_train_labels = y_train_d[0]\n",
    "\n",
    "fi1 = fisher(model, model_train_data)\n",
    "\n",
    "model_train_data = X_train_d[1].reshape(len_train_data, IMG_ROWS, IMG_COLS, 1)\n",
    "model_train_labels = y_train_d[1]\n",
    "\n",
    "fi2 = fisher(model1, model_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# if using a Jupyter notebook, include:\n",
    "%matplotlib inline\n",
    "\n",
    "d = [x.flatten() for x in fi1]\n",
    "d = np.concatenate(d)\n",
    "\n",
    "# d = a[0].flatten()\n",
    "# index = np.arange(len(d))\n",
    "# plt.bar(index, d)\n",
    "plt.plot(d)\n",
    "plt.ylabel('fisher importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = [x.flatten() for x in fi2]\n",
    "d = np.concatenate(d)\n",
    "\n",
    "# d = a[0].flatten()\n",
    "# index = np.arange(len(d))\n",
    "# plt.bar(index, d)\n",
    "plt.plot(d)\n",
    "plt.ylabel('fisher importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
