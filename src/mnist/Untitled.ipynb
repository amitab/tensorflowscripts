{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Hidden_Layer_1 (Dense)       (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_2 (Dense)       (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_3 (Dense)       (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_4 (Dense)       (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "Output_Layer (Dense)         (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 297,910\n",
      "Trainable params: 297,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training model - 0\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Epoch 1/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.6524 - accuracy: 0.7907 - val_loss: 0.2132 - val_accuracy: 0.9373\n",
      "Epoch 2/20\n",
      "252/252 [==============================] - 1s 3ms/step - loss: 0.2597 - accuracy: 0.9219 - val_loss: 0.1597 - val_accuracy: 0.9537\n",
      "Epoch 3/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.1864 - accuracy: 0.9450 - val_loss: 0.1326 - val_accuracy: 0.9612\n",
      "Epoch 4/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.1557 - accuracy: 0.9531 - val_loss: 0.1215 - val_accuracy: 0.9646\n",
      "Epoch 5/20\n",
      "252/252 [==============================] - 1s 3ms/step - loss: 0.1288 - accuracy: 0.9619 - val_loss: 0.1122 - val_accuracy: 0.9670\n",
      "Epoch 6/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.1143 - accuracy: 0.9659 - val_loss: 0.1079 - val_accuracy: 0.9705\n",
      "Epoch 7/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.1010 - accuracy: 0.9697 - val_loss: 0.1079 - val_accuracy: 0.9724\n",
      "Epoch 8/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0917 - accuracy: 0.9721 - val_loss: 0.1007 - val_accuracy: 0.9744\n",
      "Epoch 9/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0897 - accuracy: 0.9726 - val_loss: 0.1116 - val_accuracy: 0.9720\n",
      "Epoch 10/20\n",
      "252/252 [==============================] - 1s 3ms/step - loss: 0.0768 - accuracy: 0.9763 - val_loss: 0.0990 - val_accuracy: 0.9742\n",
      "Epoch 11/20\n",
      "252/252 [==============================] - 1s 3ms/step - loss: 0.0719 - accuracy: 0.9779 - val_loss: 0.0957 - val_accuracy: 0.9748\n",
      "Epoch 12/20\n",
      "252/252 [==============================] - 1s 3ms/step - loss: 0.0673 - accuracy: 0.9795 - val_loss: 0.1000 - val_accuracy: 0.9744\n",
      "Epoch 13/20\n",
      "252/252 [==============================] - 1s 3ms/step - loss: 0.0611 - accuracy: 0.9816 - val_loss: 0.1069 - val_accuracy: 0.9750\n",
      "Epoch 14/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0605 - accuracy: 0.9812 - val_loss: 0.1058 - val_accuracy: 0.9748\n",
      "Epoch 15/20\n",
      "252/252 [==============================] - 1s 3ms/step - loss: 0.0593 - accuracy: 0.9821 - val_loss: 0.1146 - val_accuracy: 0.9760\n",
      "Epoch 16/20\n",
      "252/252 [==============================] - 1s 3ms/step - loss: 0.0537 - accuracy: 0.9836 - val_loss: 0.1101 - val_accuracy: 0.9756\n",
      "Epoch 17/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0509 - accuracy: 0.9846 - val_loss: 0.1157 - val_accuracy: 0.9751\n",
      "Epoch 18/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0504 - accuracy: 0.9843 - val_loss: 0.1028 - val_accuracy: 0.9771\n",
      "Epoch 19/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0492 - accuracy: 0.9841 - val_loss: 0.1059 - val_accuracy: 0.9768\n",
      "Epoch 20/20\n",
      "252/252 [==============================] - 1s 3ms/step - loss: 0.0483 - accuracy: 0.9857 - val_loss: 0.1191 - val_accuracy: 0.9756\n",
      "Training model - 1\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Epoch 1/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.6477 - accuracy: 0.7904 - val_loss: 0.2127 - val_accuracy: 0.9364\n",
      "Epoch 2/20\n",
      "252/252 [==============================] - 1s 3ms/step - loss: 0.2601 - accuracy: 0.9237 - val_loss: 0.1572 - val_accuracy: 0.9530\n",
      "Epoch 3/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.1944 - accuracy: 0.9426 - val_loss: 0.1341 - val_accuracy: 0.9612\n",
      "Epoch 4/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.1540 - accuracy: 0.9540 - val_loss: 0.1251 - val_accuracy: 0.9645\n",
      "Epoch 5/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.1337 - accuracy: 0.9605 - val_loss: 0.1154 - val_accuracy: 0.9683\n",
      "Epoch 6/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.1147 - accuracy: 0.9663 - val_loss: 0.1109 - val_accuracy: 0.9705\n",
      "Epoch 7/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0997 - accuracy: 0.9708 - val_loss: 0.1173 - val_accuracy: 0.9707\n",
      "Epoch 8/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0907 - accuracy: 0.9739 - val_loss: 0.0993 - val_accuracy: 0.9742\n",
      "Epoch 9/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0860 - accuracy: 0.9743 - val_loss: 0.1199 - val_accuracy: 0.9715\n",
      "Epoch 10/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0783 - accuracy: 0.9758 - val_loss: 0.0990 - val_accuracy: 0.9755\n",
      "Epoch 11/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0752 - accuracy: 0.9774 - val_loss: 0.1020 - val_accuracy: 0.9739\n",
      "Epoch 12/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0692 - accuracy: 0.9804 - val_loss: 0.0987 - val_accuracy: 0.9755\n",
      "Epoch 13/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0627 - accuracy: 0.9806 - val_loss: 0.1010 - val_accuracy: 0.9763\n",
      "Epoch 14/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0610 - accuracy: 0.9816 - val_loss: 0.1077 - val_accuracy: 0.9736\n",
      "Epoch 15/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0608 - accuracy: 0.9823 - val_loss: 0.0980 - val_accuracy: 0.9761\n",
      "Epoch 16/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0534 - accuracy: 0.9845 - val_loss: 0.1031 - val_accuracy: 0.9762\n",
      "Epoch 17/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0523 - accuracy: 0.9846 - val_loss: 0.1060 - val_accuracy: 0.9751\n",
      "Epoch 18/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0491 - accuracy: 0.9852 - val_loss: 0.1034 - val_accuracy: 0.9749\n",
      "Epoch 19/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0468 - accuracy: 0.9858 - val_loss: 0.1156 - val_accuracy: 0.9735\n",
      "Epoch 20/20\n",
      "252/252 [==============================] - 1s 4ms/step - loss: 0.0460 - accuracy: 0.9858 - val_loss: 0.1029 - val_accuracy: 0.9767\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/ngbolin/mnist-dataset-digit-recognizer/data?select=sample_submission.csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1212)\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "\n",
    "df_features = df_train.iloc[:, 1:785]\n",
    "df_label = df_train.iloc[:, 0]\n",
    "\n",
    "X_test = df_test.iloc[:, 0:784]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(df_features, df_label, \n",
    "                                                test_size = 0.2,\n",
    "                                                random_state = 1212)\n",
    "\n",
    "X_train = X_train.to_numpy().reshape(33600, 784) #(33600, 784)\n",
    "X_cv = X_cv.to_numpy().reshape(8400, 784) #(8400, 784)\n",
    "\n",
    "X_test = X_test.to_numpy().reshape(28000, 784)\n",
    "\n",
    "\n",
    "# Feature Normalization \n",
    "X_train = X_train.astype('float32'); X_cv= X_cv.astype('float32'); X_test = X_test.astype('float32')\n",
    "X_train /= 255; X_cv /= 255; X_test /= 255\n",
    "\n",
    "# Convert labels to One Hot Encoded\n",
    "num_digits = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_digits)\n",
    "y_cv = keras.utils.to_categorical(y_cv, num_digits)\n",
    "\n",
    "\n",
    "shared_percent = 50 / 100\n",
    "shared_train_data_size = int(X_train.shape[0] * shared_percent)\n",
    "num_models = 2\n",
    "\n",
    "shared_train_data = X_train[:shared_train_data_size]\n",
    "shared_train_labels = y_train[:shared_train_data_size]\n",
    "\n",
    "private_train_data = np.split(X_train[shared_train_data_size:], num_models)\n",
    "private_train_labels = np.split(\n",
    "    y_train[shared_train_data_size:], num_models)\n",
    "\n",
    "# Input Parameters\n",
    "n_input = 784 # number of features\n",
    "n_hidden_1 = 300\n",
    "n_hidden_2 = 100\n",
    "n_hidden_3 = 100\n",
    "n_hidden_4 = 200\n",
    "num_digits = 10\n",
    "\n",
    "learning_rate = 0.1\n",
    "training_epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "def save_weights(model, prefix):\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if len(layer.get_weights()) == 0:\n",
    "            continue\n",
    "        m = layer.get_weights()\n",
    "        np.savetxt('saved_weights/{}-w{}.out'.format(prefix, i), m[0], header=\"{},{}\".format(*m[0].shape), delimiter=\",\")\n",
    "        np.savetxt('saved_weights/{}-b{}.out'.format(prefix, i), m[1], header=\"{},{}\".format(m[1].shape, 1), delimiter=\",\")\n",
    "\n",
    "\n",
    "main_model = keras.Sequential([\n",
    "    Input(shape=(784,)),\n",
    "    Dense(n_hidden_1, activation='relu', name = \"Hidden_Layer_1\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(n_hidden_2, activation='relu', name = \"Hidden_Layer_2\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(n_hidden_3, activation='relu', name = \"Hidden_Layer_3\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(n_hidden_4, activation='relu', name = \"Hidden_Layer_4\"),\n",
    "    Dense(num_digits, activation='softmax', name = \"Output_Layer\")\n",
    "])\n",
    "main_model.summary()\n",
    "og_file_name = 'saved_model/main_model_{}_{}.h5'.format(int(shared_percent * 100), num_models)\n",
    "main_model.save(og_file_name)\n",
    "\n",
    "for i in range(num_models):\n",
    "    print(\"Training model - {}\".format(i))\n",
    "    \n",
    "    model_train_data = np.vstack((shared_train_data, private_train_data[i]))\n",
    "    model_train_labels = np.vstack(\n",
    "        (shared_train_labels, private_train_labels[i]))\n",
    "\n",
    "    model = keras.models.load_model(og_file_name)\n",
    "    # Will compile mess up optimizer state?\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(model_train_data, model_train_labels, batch_size = batch_size, epochs = training_epochs, validation_data=(X_cv, y_cv))\n",
    "\n",
    "    test_pred = pd.DataFrame(model.predict(X_test, batch_size=200))\n",
    "    test_pred = pd.DataFrame(test_pred.idxmax(axis = 1))\n",
    "    test_pred.index.name = 'ImageId'\n",
    "    test_pred = test_pred.rename(columns = {0: 'Label'}).reset_index()\n",
    "    test_pred['ImageId'] = test_pred['ImageId'] + 1\n",
    "\n",
    "    test_pred.head()\n",
    "    \n",
    "    model_name = \"saved_model/model_{}_{}_{}.h5\".format(i, int(shared_percent * 100), num_models)\n",
    "    model.save(model_name)\n",
    "\n",
    "    save_weights(model, 'model_{}_{}_{}'.format(i, int(shared_percent * 100), num_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
